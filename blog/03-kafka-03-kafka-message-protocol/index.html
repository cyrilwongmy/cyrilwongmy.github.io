<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3) | Mingyan&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="This is part 3 of the Kafka series.

Part 1: Introduction to Kafka TCP Fragmentation
Part 2: Introduction to Reactor Pattern and Kafka&rsquo;s Reactor Implementation

The source code referenced in this article uses Kafka&rsquo;s trunk branch. I&rsquo;ve pushed it to my personal repo for version alignment with this article:
https://github.com/cyrilwongmy/kafka-src-reading
In this part, we will briefly introduce the Kafka&rsquo;s message protocol. Kafka’s server and client communicate through a custom message protocol. By understanding how Kafka’s RPC protocol is implemented, we can apply similar principles when designing our own protocols.">
<meta name="author" content="">
<link rel="canonical" href="https://cyrilwongmy.github.io/blog/03-kafka-03-kafka-message-protocol/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://cyrilwongmy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://cyrilwongmy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://cyrilwongmy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://cyrilwongmy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://cyrilwongmy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://cyrilwongmy.github.io/blog/03-kafka-03-kafka-message-protocol/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://cyrilwongmy.github.io/blog/03-kafka-03-kafka-message-protocol/">
  <meta property="og:site_name" content="Mingyan&#39;s blog">
  <meta property="og:title" content="An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3)">
  <meta property="og:description" content="This is part 3 of the Kafka series.
Part 1: Introduction to Kafka TCP Fragmentation Part 2: Introduction to Reactor Pattern and Kafka’s Reactor Implementation The source code referenced in this article uses Kafka’s trunk branch. I’ve pushed it to my personal repo for version alignment with this article:
https://github.com/cyrilwongmy/kafka-src-reading
In this part, we will briefly introduce the Kafka’s message protocol. Kafka’s server and client communicate through a custom message protocol. By understanding how Kafka’s RPC protocol is implemented, we can apply similar principles when designing our own protocols.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-06-06T23:11:18-07:00">
    <meta property="article:modified_time" content="2025-06-06T23:11:18-07:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3)">
<meta name="twitter:description" content="This is part 3 of the Kafka series.

Part 1: Introduction to Kafka TCP Fragmentation
Part 2: Introduction to Reactor Pattern and Kafka&rsquo;s Reactor Implementation

The source code referenced in this article uses Kafka&rsquo;s trunk branch. I&rsquo;ve pushed it to my personal repo for version alignment with this article:
https://github.com/cyrilwongmy/kafka-src-reading
In this part, we will briefly introduce the Kafka&rsquo;s message protocol. Kafka’s server and client communicate through a custom message protocol. By understanding how Kafka’s RPC protocol is implemented, we can apply similar principles when designing our own protocols.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://cyrilwongmy.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3)",
      "item": "https://cyrilwongmy.github.io/blog/03-kafka-03-kafka-message-protocol/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3)",
  "name": "An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3)",
  "description": "This is part 3 of the Kafka series.\nPart 1: Introduction to Kafka TCP Fragmentation Part 2: Introduction to Reactor Pattern and Kafka\u0026rsquo;s Reactor Implementation The source code referenced in this article uses Kafka\u0026rsquo;s trunk branch. I\u0026rsquo;ve pushed it to my personal repo for version alignment with this article:\nhttps://github.com/cyrilwongmy/kafka-src-reading\nIn this part, we will briefly introduce the Kafka\u0026rsquo;s message protocol. Kafka’s server and client communicate through a custom message protocol. By understanding how Kafka’s RPC protocol is implemented, we can apply similar principles when designing our own protocols.\n",
  "keywords": [
    
  ],
  "articleBody": "This is part 3 of the Kafka series.\nPart 1: Introduction to Kafka TCP Fragmentation Part 2: Introduction to Reactor Pattern and Kafka’s Reactor Implementation The source code referenced in this article uses Kafka’s trunk branch. I’ve pushed it to my personal repo for version alignment with this article:\nhttps://github.com/cyrilwongmy/kafka-src-reading\nIn this part, we will briefly introduce the Kafka’s message protocol. Kafka’s server and client communicate through a custom message protocol. By understanding how Kafka’s RPC protocol is implemented, we can apply similar principles when designing our own protocols.\nIntroduction HTTP 1.x and gRPC are convenient, but too heavyweight for a high‑throughput, low‑latency durable log. Each HTTP header on a TCP frame adds unnecessary bytes to every message. Even Protobuf’s var‑int encoding becomes a bottleneck when allocating thousands of objects per millisecond, putting strain on heap memory and garbage collection.\nThe Apache community’s “don’t break users” philosophy requires brokers and clients to communicate across major versions. Their solution: a custom protocol with tagged fields that allows new data to be appended to message ends. Like HTTP/2 frames, older clients can safely ignore unknown tags.\nSerialization and Deserialization Mechanisms At their core, all application-layer protocols—whether it’s RPC or HTTP—follow a similar idea: converting in-memory objects into binary form for network transmission. On the receiving end, the system reads this binary stream and reconstructs the original in-memory structures by parsing it piece by piece.\nKafka Protocol Request Structure All request types in Kafka are located under the org.apache.kafka.common.requests package. Let’s use ProduceRequest as an example to walk through its protocol implementation.\nAnatomy of ProduceRequest To understand the protocol, we’ll look at a simplified version for clarity. As shown in the diagram, the network representation of a ProduceRequest is a deeply nested object structure—much like the layered model in computer networking.\nLevel Field Type Meaning Example 0 acks INT16 Required replica acknowledgements ‑1 (all ISRs) 0 timeoutMs INT32 Hard write timeout (ms) 30000 0 topicData.length INT32 Number of TopicProduceData 2 1 name STRING Topic name \"orders\" 1 partitionData.length INT32 Number of PartitionProduceData 3 2 index INT32 Partition index 0 2 recordSetSize INT32 Size of the record set (bytes) 1024 2 recordSet BYTES Compressed batch of messages … The outermost object is ProduceRequestData, which contains configuration info and the number of topic entries. Why record the number of topics? Because the receiver must know how many TopicProduceData entries to expect—otherwise, it can’t determine message boundaries. For unknown-length sequences, you must explicitly record the size, typically using 2 or 4 bytes. The receiver reads those bytes and converts them into int or short to know how many elements to parse next. A single ProduceRequest can target multiple topics, each represented by a TopicProduceData object. Each TopicProduceData includes: the byte length and content of the topic name, the number of partitions, and a list of PartitionProduceData. Each PartitionProduceData records:\nthe partition index, the length of the record batch, and the actual set of messages. Kafka’s Serialization Implementation Kafka uses code generation to implement serialization and deserialization. This helps avoid typos and ensures compatibility across protocol versions. A global ObjectSerializationCache is used to cache UTF-8 encoded strings and avoid recomputation.\nWhen serializing a ProduceRequest, Kafka calls the write method from ProduceRequestData, which implements the Message interface. This method serializes the in-memory object into a Writable—usually a wrapper around Java NIO’s ByteBuffer, like ByteBufferAccessor.\nThe write method contains many version checks to ensure forward/backward compatibility, which is critical when designing a robust protocol.\nKey fields written during serialization: acks: how many replicas must confirm the write. timeoutMs: how long the broker should wait for replicas to acknowledge. topicData.size(): number of topics in the list (not byte length). Each topic’s data is serialized via its own write method. Why Length Fields Matter? In OSI Layer 4, TCP guarantees byte‑stream ordering but not message boundaries. This means that without explicit size indicators, a receiver cannot determine where one logical request ends and another begins. As a result, every ARRAY / STRING / BYTES field in ProduceRequest follows a length → payload structure.\nHere’s a simplified version of the write method from TopicProduceData:\npublic void write(Writable writable, ObjectSerializationCache cache, short version) { byte[] stringBytes = cache.getSerializedValue(name); if (version \u003e= 9) { writable.writeUnsignedVarint(stringBytes.length + 1); } else { writable.writeShort((short) stringBytes.length); } writable.writeByteArray(stringBytes); if (version \u003e= 9) { writable.writeUnsignedVarint(partitionData.size() + 1); } else { writable.writeInt(partitionData.size()); } for (PartitionProduceData p : partitionData) { p.write(writable, cache, version); } } This recursive structure ensures that all nested fields are serialized correctly. Ultimately, all data ends up in a ByteBuffer, which Kafka sends over the network.\nOptimization: Zero-Copy For high-performance messaging, Kafka avoids loading entire files into memory before sending. It uses zero-copy techniques to minimize unnecessary memory copies.\nThe usual data flow from disk to network is:\nDisk -\u003e Kernel Space -\u003e User Space -\u003e Socket Buffer -\u003e Network\nKafka uses zero-copy to avoid the memory copy from the kernel space to the user space:\nDisk -\u003e Kernel Space -\u003e Network\nWe’ll dive deeper into this in a future blog about Kafka’s zero-copy optimizations.\nKafka’s Deserialization Implementation Deserialization is handled by the read method in the Message interface, which reads from a Readable (usually a buffer).\nHere’s how ProduceRequestData.read() works:\nRead 2 bytes → convert to short for acks Read 4 bytes → convert to int for timeoutMs Read 4 bytes → get number of TopicProduceData entries For each topic, invoke its read method to deserialize nested structures Example:\npublic void read(Readable readable, short version) { this.acks = readable.readShort(); this.timeoutMs = readable.readInt(); int arrayLength = readable.readInt(); TopicProduceDataCollection collection = new TopicProduceDataCollection(arrayLength); for (int i = 0; i \u003c arrayLength; i++) { collection.add(new TopicProduceData(readable, version)); } this.topicData = collection; } This is essentially the reverse of the write method.\nTakeway about forward/backward compatibility Scenario Strategy Benefit Add a field Append to tail or use a tagged field Old clients ignore it Remove field Markdeprecated, keep parser Old data can still replay Widen field Dual‑write / dual‑read, then retire old Online upgrades stay safe Conclusion Implementing a simple RPC-like protocol is quite approachable. From Kafka’s design, we learn how to use recursive serialization/deserialization effectively.\nFor further optimization, consider:\nCompressing data to reduce transmission size, Reducing memory copies when transmitting from disk or in-memory buffers to the network (e.g., using zero-copy). These principles form a solid foundation for building efficient and maintainable binary protocols.\nIf you have any question, free feel to leave issues at this repo.\n",
  "wordCount" : "1068",
  "inLanguage": "en",
  "datePublished": "2025-06-06T23:11:18-07:00",
  "dateModified": "2025-06-06T23:11:18-07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cyrilwongmy.github.io/blog/03-kafka-03-kafka-message-protocol/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mingyan's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cyrilwongmy.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://cyrilwongmy.github.io/" accesskey="h" title="Mingyan&#39;s blog (Alt + H)">Mingyan&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      An introduction to Kafka’s High‑Performance Binary RPC Protocol (Part 3)
    </h1>
    <div class="post-meta"><span title='2025-06-06 23:11:18 -0700 PDT'>June 6, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>This is part 3 of the Kafka series.</p>
<ul>
<li><a href="https://cyrilwongmy.github.io/blog/01-kafka-01-tcp-fragmentation/">Part 1: Introduction to Kafka TCP Fragmentation</a></li>
<li><a href="https://cyrilwongmy.github.io/blog/02-kafka-02-kafka-reactor/">Part 2: Introduction to Reactor Pattern and Kafka&rsquo;s Reactor Implementation</a></li>
</ul>
<p>The source code referenced in this article uses Kafka&rsquo;s trunk branch. I&rsquo;ve pushed it to my personal repo for version alignment with this article:</p>
<p><a href="https://github.com/cyrilwongmy/kafka-src-reading">https://github.com/cyrilwongmy/kafka-src-reading</a></p>
<p>In this part, we will briefly introduce the Kafka&rsquo;s message protocol. Kafka’s server and client communicate through a custom message protocol. By understanding how Kafka’s RPC protocol is implemented, we can apply similar principles when designing our own protocols.</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>HTTP 1.x and gRPC are convenient, but too heavyweight for a <strong>high‑throughput, low‑latency <em>durable</em> log</strong>. Each HTTP header on a TCP frame adds unnecessary bytes to every message. Even Protobuf&rsquo;s var‑int encoding becomes a bottleneck when allocating thousands of objects per millisecond, putting strain on heap memory and garbage collection.</p>
<p>The Apache community&rsquo;s &ldquo;<strong>don&rsquo;t break users</strong>&rdquo; philosophy requires brokers and clients to communicate across major versions. Their solution: a custom protocol with <strong>tagged fields</strong> that allows new data to be appended to message ends. Like HTTP/2 frames, older clients can safely ignore unknown tags.</p>
<h2 id="serialization-and-deserialization-mechanisms"><strong>Serialization and Deserialization Mechanisms</strong><a hidden class="anchor" aria-hidden="true" href="#serialization-and-deserialization-mechanisms">#</a></h2>
<p>At their core, all application-layer protocols—whether it’s RPC or HTTP—follow a similar idea: converting in-memory objects into binary form for network transmission. On the receiving end, the system reads this binary stream and reconstructs the original in-memory structures by parsing it piece by piece.</p>
<h1 id="kafka-protocol-request-structure"><strong>Kafka Protocol Request Structure</strong><a hidden class="anchor" aria-hidden="true" href="#kafka-protocol-request-structure">#</a></h1>
<p>All request types in Kafka are located under the <code>org.apache.kafka.common.requests</code> package. Let’s use <code>ProduceRequest</code> as an example to walk through its protocol implementation.</p>
<h2 id="anatomy-of-producerequest">Anatomy of <code>ProduceRequest</code><a hidden class="anchor" aria-hidden="true" href="#anatomy-of-producerequest">#</a></h2>
<p>To understand the protocol, we’ll look at a simplified version for clarity. As shown in the diagram, the network representation of a ProduceRequest is a deeply nested object structure—much like the layered model in computer networking.</p>
<p><img alt="image.png" loading="lazy" src="https://raw.githubusercontent.com/cyrilwongmy/cyrilwongmy.github.io/refs/heads/main/assets/image/message-protocol-image.png"></p>
<table>
  <thead>
      <tr>
          <th>Level</th>
          <th>Field</th>
          <th>Type</th>
          <th>Meaning</th>
          <th>Example</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0</td>
          <td><code>acks</code></td>
          <td>INT16</td>
          <td>Required replica acknowledgements</td>
          <td><code>‑1</code> (all ISRs)</td>
      </tr>
      <tr>
          <td>0</td>
          <td><code>timeoutMs</code></td>
          <td>INT32</td>
          <td>Hard write timeout (ms)</td>
          <td><code>30000</code></td>
      </tr>
      <tr>
          <td>0</td>
          <td><code>topicData.length</code></td>
          <td>INT32</td>
          <td>Number of <code>TopicProduceData</code></td>
          <td><code>2</code></td>
      </tr>
      <tr>
          <td>1</td>
          <td><code>name</code></td>
          <td>STRING</td>
          <td>Topic name</td>
          <td><code>&quot;orders&quot;</code></td>
      </tr>
      <tr>
          <td>1</td>
          <td><code>partitionData.length</code></td>
          <td>INT32</td>
          <td>Number of <code>PartitionProduceData</code></td>
          <td><code>3</code></td>
      </tr>
      <tr>
          <td>2</td>
          <td><code>index</code></td>
          <td>INT32</td>
          <td>Partition index</td>
          <td><code>0</code></td>
      </tr>
      <tr>
          <td>2</td>
          <td><code>recordSetSize</code></td>
          <td>INT32</td>
          <td>Size of the record set (bytes)</td>
          <td><code>1024</code></td>
      </tr>
      <tr>
          <td>2</td>
          <td><code>recordSet</code></td>
          <td>BYTES</td>
          <td>Compressed batch of messages</td>
          <td>…</td>
      </tr>
  </tbody>
</table>
<ul>
<li>The outermost object is ProduceRequestData, which contains configuration info and the number of topic entries.</li>
<li>Why record the number of topics? Because the receiver must know how many TopicProduceData entries to expect—otherwise, it can’t determine message boundaries. For unknown-length sequences, you must explicitly record the size, typically using 2 or 4 bytes. The receiver reads those bytes and converts them into int or short to know how many elements to parse next.</li>
<li>A single ProduceRequest can target multiple topics, each represented by a TopicProduceData object.</li>
<li>Each TopicProduceData includes:
<ul>
<li>the byte length and content of the topic name,</li>
<li>the number of partitions,</li>
<li>and a list of PartitionProduceData.</li>
</ul>
</li>
</ul>
<p>Each PartitionProduceData records:</p>
<ul>
<li>the partition index,</li>
<li>the length of the record batch,</li>
<li>and the actual set of messages.</li>
</ul>
<h2 id="kafkas-serialization-implementation"><strong>Kafka’s Serialization Implementation</strong><a hidden class="anchor" aria-hidden="true" href="#kafkas-serialization-implementation">#</a></h2>
<p>Kafka uses <strong>code generation</strong> to implement serialization and deserialization. This helps avoid typos and ensures compatibility across protocol versions. A global ObjectSerializationCache is used to cache UTF-8 encoded strings and avoid recomputation.</p>
<p>When serializing a ProduceRequest, Kafka calls the write method from ProduceRequestData, which implements the Message interface. This method serializes the in-memory object into a Writable—usually a wrapper around Java NIO’s ByteBuffer, like ByteBufferAccessor.</p>
<p>The write method contains many version checks to ensure forward/backward compatibility, which is critical when designing a robust protocol.</p>
<h2 id="key-fields-written-during-serialization"><strong>Key fields written during serialization:</strong><a hidden class="anchor" aria-hidden="true" href="#key-fields-written-during-serialization">#</a></h2>
<ul>
<li>acks: how many replicas must confirm the write.</li>
<li>timeoutMs: how long the broker should wait for replicas to acknowledge.</li>
<li>topicData.size(): number of topics in the list (not byte length).</li>
<li>Each topic’s data is serialized via its own write method.</li>
</ul>
<p>Why Length Fields Matter? In OSI Layer 4, TCP guarantees <strong>byte‑stream ordering</strong> but not <em>message</em> boundaries. This means that without explicit size indicators, a receiver cannot determine where one logical request ends and another begins. As a result, every <strong>ARRAY / STRING / BYTES</strong> field in <code>ProduceRequest</code> follows a <em>length → payload</em> structure.</p>
<p>Here’s a simplified version of the write method from TopicProduceData:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-java" data-lang="java"><span style="display:flex;"><span><span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">write</span>(Writable writable, ObjectSerializationCache cache, <span style="color:#66d9ef">short</span> version) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">byte</span><span style="color:#f92672">[]</span> stringBytes <span style="color:#f92672">=</span> cache.<span style="color:#a6e22e">getSerializedValue</span>(name);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (version <span style="color:#f92672">&gt;=</span> 9) {
</span></span><span style="display:flex;"><span>        writable.<span style="color:#a6e22e">writeUnsignedVarint</span>(stringBytes.<span style="color:#a6e22e">length</span> <span style="color:#f92672">+</span> 1);
</span></span><span style="display:flex;"><span>    } <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>        writable.<span style="color:#a6e22e">writeShort</span>((<span style="color:#66d9ef">short</span>) stringBytes.<span style="color:#a6e22e">length</span>);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    writable.<span style="color:#a6e22e">writeByteArray</span>(stringBytes);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (version <span style="color:#f92672">&gt;=</span> 9) {
</span></span><span style="display:flex;"><span>        writable.<span style="color:#a6e22e">writeUnsignedVarint</span>(partitionData.<span style="color:#a6e22e">size</span>() <span style="color:#f92672">+</span> 1);
</span></span><span style="display:flex;"><span>    } <span style="color:#66d9ef">else</span> {
</span></span><span style="display:flex;"><span>        writable.<span style="color:#a6e22e">writeInt</span>(partitionData.<span style="color:#a6e22e">size</span>());
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (PartitionProduceData p : partitionData) {
</span></span><span style="display:flex;"><span>        p.<span style="color:#a6e22e">write</span>(writable, cache, version);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>This recursive structure ensures that all nested fields are serialized correctly. Ultimately, all data ends up in a ByteBuffer, which Kafka sends over the network.</p>
<h2 id="optimization-zero-copy"><strong>Optimization: Zero-Copy</strong><a hidden class="anchor" aria-hidden="true" href="#optimization-zero-copy">#</a></h2>
<p>For high-performance messaging, Kafka avoids loading entire files into memory before sending. It uses <strong>zero-copy</strong> techniques to minimize unnecessary memory copies.</p>
<p>The usual data flow from disk to network is:</p>
<p>Disk -&gt; Kernel Space -&gt; User Space -&gt; Socket Buffer -&gt; Network</p>
<p>Kafka uses zero-copy to avoid the memory copy from the kernel space to the user space:</p>
<p>Disk -&gt; Kernel Space -&gt; Network</p>
<p>We’ll dive deeper into this in a future blog about Kafka’s zero-copy optimizations.</p>
<h2 id="kafkas-deserialization-implementation"><strong>Kafka’s Deserialization Implementation</strong><a hidden class="anchor" aria-hidden="true" href="#kafkas-deserialization-implementation">#</a></h2>
<p>Deserialization is handled by the read method in the Message interface, which reads from a Readable (usually a buffer).</p>
<p>Here’s how ProduceRequestData.read() works:</p>
<ul>
<li>Read 2 bytes → convert to short for acks</li>
<li>Read 4 bytes → convert to int for timeoutMs</li>
<li>Read 4 bytes → get number of TopicProduceData entries</li>
<li>For each topic, invoke its read method to deserialize nested structures</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-java" data-lang="java"><span style="display:flex;"><span> <span style="color:#66d9ef">public</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">read</span>(Readable readable, <span style="color:#66d9ef">short</span> version) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">this</span>.<span style="color:#a6e22e">acks</span> <span style="color:#f92672">=</span> readable.<span style="color:#a6e22e">readShort</span>();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">this</span>.<span style="color:#a6e22e">timeoutMs</span> <span style="color:#f92672">=</span> readable.<span style="color:#a6e22e">readInt</span>();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> arrayLength <span style="color:#f92672">=</span> readable.<span style="color:#a6e22e">readInt</span>();
</span></span><span style="display:flex;"><span>    TopicProduceDataCollection collection <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> TopicProduceDataCollection(arrayLength);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> 0; i <span style="color:#f92672">&lt;</span> arrayLength; i<span style="color:#f92672">++</span>) {
</span></span><span style="display:flex;"><span>        collection.<span style="color:#a6e22e">add</span>(<span style="color:#66d9ef">new</span> TopicProduceData(readable, version));
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">this</span>.<span style="color:#a6e22e">topicData</span> <span style="color:#f92672">=</span> collection;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>This is essentially the reverse of the write method.</p>
<h2 id="takeway-about-forwardbackward-compatibility">Takeway about forward/backward compatibility<a hidden class="anchor" aria-hidden="true" href="#takeway-about-forwardbackward-compatibility">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Scenario</th>
          <th>Strategy</th>
          <th>Benefit</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Add a field</td>
          <td>Append to tail or use a tagged field</td>
          <td>Old clients ignore it</td>
      </tr>
      <tr>
          <td>Remove field</td>
          <td>Mark<em>deprecated</em>, keep parser</td>
          <td>Old data can still replay</td>
      </tr>
      <tr>
          <td>Widen field</td>
          <td>Dual‑write / dual‑read, then retire old</td>
          <td>Online upgrades stay safe</td>
      </tr>
  </tbody>
</table>
<h1 id="conclusion"><strong>Conclusion</strong><a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>Implementing a simple RPC-like protocol is quite approachable. From Kafka’s design, we learn how to use <strong>recursive serialization/deserialization</strong> effectively.</p>
<p>For further optimization, consider:</p>
<ul>
<li>Compressing data to reduce transmission size,</li>
<li>Reducing memory copies when transmitting from disk or in-memory buffers to the network (e.g., using zero-copy).</li>
</ul>
<p>These principles form a solid foundation for building efficient and maintainable binary protocols.</p>
<p>If you have any question, free feel to leave issues at this <a href="https://github.com/cyrilwongmy/kafka-src-reading">repo</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://cyrilwongmy.github.io/">Mingyan&#39;s blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
